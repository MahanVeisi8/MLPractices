# -*- coding: utf-8 -*-
"""MLpractice2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19gz7e82r64mSo6nlEzNGXp4zhjXDt45W

#**Welcome to the second practice of ML ðŸ˜‡**
##I am Mahan Veisi and I hope you enjoy reading this machine learning notebook.

#**Q1**

###A stroke, also known as a cerebrovascular accident or CVA, occurs when a part of the brain is deprived of its blood supply and the part of the body that is controlled by brain cells deprived of blood stops working. This loss of blood supply can be due to lack of blood flow or due to bleeding in the brain tissue. A stroke is a medical emergency because it can lead to death or permanent disability. There are possibilities to treat these types of strokes, but this treatment should be started in the first few hours after the appearance of stroke symptoms.

###The ***csv.strokes*** data set contains the information of people and their stroke history. In this case, do the following steps:

##**a)** Perform pre-processing operations according to the goal of the problem.

importing the data
"""

import pandas as pd
import requests
from io import StringIO

dataset_link = 'https://physionet.org/static/published-projects/gaitpdb/gait-in-parkinsons-disease-1.0.0.zip'

# Download the file
response = requests.get(dataset_link)
data = response.text

# Use pandas to read the data (assuming it's a CSV file, adjust as needed)
df = pd.read_csv(StringIO(data))

# Now, 'df' contains your dataset
print(df.head())

import pandas as pd

csv = 'dataset.csv'
data = pd.read_csv(csv)

data

"""The Id feature is not gonna help us in predication of a stroke so I drop it. Aslo I I divide the data into two sections, X and Y"""

X = data.drop(['id', 'ever_married', 'stroke'], axis=1)

y = data['stroke']

"""Let's check our data's shape"""

print('X.shape: ' + str(X.shape))
print('y.shape: ' + str(y.shape))

"""but we may have missing values in our data"""

if data.isnull().values.any():
    print('The dataset has missing values.')
else:
    print('The dataset has no missing values.')

missing_values_per_feature = data.isnull().sum()

if missing_values_per_feature.any():
    print('Features with missing values:')
    print(missing_values_per_feature[missing_values_per_feature > 0])
else:
    print('No missing values in any feature.')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer


numericals = ['age', 'avg_glucose_level', 'bmi', 'hypertension', 'heart_disease']
numeric_imputer = SimpleImputer(strategy='mean')
X[numericals] = numeric_imputer.fit_transform(X[numericals])
scaler = StandardScaler()
X[numericals] = scaler.fit_transform(X[numericals])

categoricals = ['gender', 'work_type', 'Residence_type', 'smoking_status']
categorical_imputer = SimpleImputer(strategy='most_frequent')
X[categoricals] = categorical_imputer.fit_transform(X[categoricals])
encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
X_encoded = encoder.fit_transform(X[categoricals])

X = X.drop(categoricals, axis=1)
X = pd.concat([X, pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(categoricals))], axis=1)

X

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

model = SVC(kernel='linear', class_weight='balanced', random_state=42)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print('Confusion Matrix:\n', conf_matrix)
print('Classification Report:\n', classification_rep)

"""very strange!

if you look at the report, our f1-score for ones is only 0.1 and it is really bad for detecting strokes!

Let's look at our data again
"""

X

"""So after all these efforts, our model can not predicate well. Actually, our data aren't very nice either! In addition, the zero data algorithm has a great deal of diversity.

We start again and this time we only use 2 * len(**one labeled data**) of the zero labeled dataset. This way, we can have a better dataset to feed to out model
The rest is like the previous tasks we've done
"""

majority_zero = data[data['stroke'] == 0]
minority_one = data[data['stroke'] == 1]

print("majority_zero.shape: " + str(majority_zero.shape))
print("minority_one.shape: " + str(minority_one.shape))

"""Let's start the process of sampling!"""

undersampled_majority = majority_zero.sample(n=len(minority_one)*2, random_state=42)
balanced_data = pd.concat([undersampled_majority, minority_one])
balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)

X = balanced_data.drop(['id', 'ever_married', 'stroke'], axis=1)
y = balanced_data['stroke']

numericals = ['age', 'avg_glucose_level', 'bmi', 'hypertension', 'heart_disease']
numeric_imputer = SimpleImputer(strategy='mean')
X[numericals] = numeric_imputer.fit_transform(X[numericals])
scaler = StandardScaler()
X[numericals] = scaler.fit_transform(X[numericals])

categoricals = ['gender', 'work_type', 'Residence_type', 'smoking_status']
categorical_imputer = SimpleImputer(strategy='most_frequent')
X[categoricals] = categorical_imputer.fit_transform(X[categoricals])
encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
X_encoded = encoder.fit_transform(X[categoricals])

X = X.drop(categoricals, axis=1)
X = pd.concat([X, pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(categoricals))], axis=1)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

model = SVC(kernel='linear', class_weight='balanced', random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print('Confusion Matrix:\n', conf_matrix)
print('Classification Report:\n', classification_rep)

"""Wow! ðŸ™‚

Much better. Now we have 0.85 of recall for ones.

We also can use poly SVCs. lowering the threshold for predicting ones is anouther way to test to see if we can have better results.
"""

model_poly = SVC(kernel='poly', degree=3, class_weight='balanced', random_state=42)
model_poly.fit(X_train, y_train)
y_pred_poly = model_poly.predict(X_test)
classification_rep_poly = classification_report(y_test, y_pred_poly)
print('Classification Report (Polynomial Kernel):\n', classification_rep_poly)

"""So linear seems to be better in this case

results are acceptable but lets set a threshold so that we can predicate more risky strokes
"""

model_poly = SVC(kernel='linear', class_weight='balanced', probability=True, random_state=42)
model_poly.fit(X_train, y_train)
y_pred_poly_proba = model_poly.predict_proba(X_test)[:, 1]

custom_threshold = 0.3
y_pred_poly_thresholded = (y_pred_poly_proba > custom_threshold).astype(int)

classification_rep_poly_thresholded = classification_report(y_test, y_pred_poly_thresholded)
print('Classification Report (3rd-degree Poly SVC with Threshold Adjustment):\n', classification_rep_poly_thresholded)

"""but which threshold is the best one? let's find it using our new function **custom_scorer**!

As we are detecting stroke, it is very important not to miss the ones, so I have set 90% of our attention to F1 score of ones in my custom function.
"""

from sklearn.metrics import f1_score, recall_score
from sklearn.svm import SVC
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report

def custom_scorer(y_true, y_pred):
    f1_ones = f1_score(y_true, y_pred, pos_label=1)
    f1_zeros = f1_score(y_true, y_pred, pos_label=0)
    recall_ones = recall_score(y_true, y_pred, pos_label=1)
    recall_zeros = recall_score(y_true, y_pred, pos_label=0)

    return 0.9 * f1_ones + 0.1 * f1_zeros, 0.9 * recall_ones + 0.1 * recall_zeros

model_poly = SVC(kernel='linear', class_weight='balanced', probability=True, random_state=42)
model_poly.fit(X_train, y_train)

custom_thresholds = [0.05, 0.1, 0.15, 0.20, 0.25, 0.30, 0.35, 0.4]
best_threshold = custom_thresholds[0]
best_score_f1, best_score_recall = 0, 0

history_scores_f1 = []
history_scores_recall = []

for threshold in custom_thresholds:
    y_pred_poly_proba = model_poly.predict_proba(X_test)[:, 1]
    pred = (y_pred_poly_proba > threshold).astype(int)
    score_f1, score_recall = custom_scorer(y_test, pred)
    history_scores_f1.append(score_f1)
    history_scores_recall.append(score_recall)

    if score_f1 > best_score_f1:
        best_threshold = threshold
        best_score_f1 = score_f1
        best_score_recall = score_recall
        best_pred = pred

print(f"Best F1 Score: {best_score_f1}")
print(f"Best Recall Score: {best_score_recall}")
print(f"Best Threshold: {best_threshold}")
print("Classification Report for Best Threshold:")
print(classification_report(y_test, best_pred))

plt.plot(custom_thresholds, history_scores_f1, label='F1 Score')
plt.plot(custom_thresholds, history_scores_recall, label='Recall Score')
plt.title('Custom Scores over Thresholds')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.legend()
plt.show()

"""##Q2"""

ls

import pandas as pd
csv = 'insurance.csv'
data = pd.read_csv(csv)
print(data.head())

print("shape: " + str(data.shape))

print("Missing values:\n", data.isnull().sum())

"""that's great! We don't need to worry about having missing values in each row."""

data

data = pd.get_dummies(data, columns=['sex', 'smoker', 'region'])
numeric_features = ['age', 'bmi', 'children', 'charges']
scaler = StandardScaler()
data[numeric_features] = scaler.fit_transform(data[numeric_features])

data.head()

from sklearn.model_selection import train_test_split

X = data.drop('charges', axis=1)
y = data['charges']

print("X.shape: ", str(X.shape))
print("y.shape: ", str(y.shape))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Shapes after Train-Test Split:")
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""so let's create the linear regression model from scratch.

some important notes:



*   to prevent unbias problems after the initializing (for example when all of out inputs are zeros and we want to predict the result), I use np.ones() to bias my input. so finally it concatinates the ones to my features! isn't it beautiful? ðŸ™‚
*   I use Mean Squared Error (MSE) for the loss function
*   learning rate is set to 0.01 but we should test what value can be better (definitely Variable learning rates are better but it is beyond the scope of this HW)
*   as we have used bias for trainig, we should use it for our predictions too!
*   I also liked to add early stopping so that if the loss doesn't decrease or decreases very slowly, you can stop the training process early ðŸ¤”
"""

import numpy as np
import matplotlib.pyplot as plt

class LinearRegression:
    def __init__(self, learning_rate=0.01, epochs=1000, early_stopping_epochs=50):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.early_stopping_epochs = early_stopping_epochs
        self.coefficients = None
        self.loss_history = []

    def mean_squared_error(self, y_true, y_pred):
        return np.mean((y_true - y_pred)**2)

    def fit(self, X, y):
        X_with_bias = np.c_[np.ones(X.shape[0]), X]
        self.coefficients = np.zeros(X_with_bias.shape[1])

        prev_loss = float('inf')
        consecutive_epochs_no_improvement = 0

        for epoch in range(self.epochs):
            predictions = np.dot(X_with_bias, self.coefficients)
            errors = predictions - y
            gradients = 2 * np.dot(errors, X_with_bias) / X_with_bias.shape[0]
            self.coefficients -= self.learning_rate * gradients
            current_loss = self.mean_squared_error(y, predictions)
            self.loss_history.append(current_loss)

            if epoch % 100 == 0:
                print(f'Epoch {epoch}, Loss: {current_loss}')

            # early stopping
            if current_loss >= prev_loss:
                consecutive_epochs_no_improvement += 1
                if consecutive_epochs_no_improvement >= self.early_stopping_epochs:
                    print(f"Stopping early at epoch {epoch} due to lack of improvement.")
                    break
            else:
                consecutive_epochs_no_improvement = 0

            prev_loss = current_loss

    def predict(self, X):
        X_with_bias = np.c_[np.ones(X.shape[0]), X]
        predictions = np.dot(X_with_bias, self.coefficients)
        return predictions

    def plot_loss_history(self):
        plt.plot(range(1, len(self.loss_history) + 1), self.loss_history, marker='o', linestyle='-', color='b')
        plt.title('Training Loss over Epochs')
        plt.xlabel('Epoch')
        plt.ylabel('Mean Squared Error (MSE)')
        plt.show()

linear_model = LinearRegression(learning_rate=0.001, epochs=100000, early_stopping_epochs=50)
linear_model.fit(X_train, y_train)
predictions_linear = linear_model.predict(X_test)
mse_linear = np.mean((y_test - predictions_linear)**2)
accuracy_linear = 1 - (mse_linear / np.var(y_test))

print(f'Linear Regression Model Accuracy: {accuracy_linear}')

linear_model.plot_loss_history()

"""IT IS SO COOL HA? ðŸ”¥"""

plt.figure(figsize=(8, 8))
plt.scatter(y_test, predictions_linear, alpha=0.5)
plt.title('Actual vs. Predicted Values')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.show()

"""now let's use polynomial features to the input X.

so I create a second model with some functions in it to
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures

class PolynomialRegression:
    def __init__(self, degree=1, learning_rate=0.01, epochs=1000):
        self.degree = degree
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.coefficients = None
        self.loss_history = None
        self.poly_features = PolynomialFeatures(degree=degree)

    def add_polynomial_features(self, X):
        return self.poly_features.fit_transform(X)

    def mean_squared_error(self, y_true, y_pred):
        return np.mean((y_true - y_pred)**2)

    def fit(self, X, y):
        X_poly = self.add_polynomial_features(X)
        X_with_bias = np.c_[np.ones(X_poly.shape[0]), X_poly]
        self.coefficients = np.zeros(X_with_bias.shape[1])
        self.loss_history = []

        for epoch in range(self.epochs):
            predictions = np.dot(X_with_bias, self.coefficients)
            errors = predictions - y

            gradients = 2 * np.dot(errors, X_with_bias) / X_with_bias.shape[0]
            self.coefficients -= self.learning_rate * gradients

            current_loss = self.mean_squared_error(y, predictions)
            self.loss_history.append(current_loss)

            if epoch % 100 == 0:
                print(f'Epoch {epoch}, Loss: {current_loss}')

    def predict(self, X):
        X_poly = self.add_polynomial_features(X)
        X_with_bias = np.c_[np.ones(X_poly.shape[0]), X_poly]
        return np.dot(X_with_bias, self.coefficients)

    def plot_loss_history(self):
        plt.plot(range(1, self.epochs + 1), self.loss_history, marker='o', linestyle='-', color='b')
        plt.title('Training Loss over Epochs')
        plt.xlabel('Epoch')
        plt.ylabel('Mean Squared Error (MSE)')
        plt.show()

poly_model = PolynomialRegression(degree=2, learning_rate=0.01, epochs=20000)
poly_model.fit(X_train, y_train)
predictions_poly = poly_model.predict(X_test)
mse_poly = np.mean((y_test - predictions_poly)**2)
accuracy_poly = 1 - (mse_poly / np.var(y_test))

print(f'Polynomial Regression Model Accuracy: {accuracy_poly}')

poly_model.plot_loss_history()

plt.figure(figsize=(8, 8))
plt.scatter(y_test, predictions_poly, alpha=0.5)
plt.title('Actual vs. Predicted Values (Polynomial Regression)')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.show()

"""so the polynomial works better even in less epochs ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥"""

