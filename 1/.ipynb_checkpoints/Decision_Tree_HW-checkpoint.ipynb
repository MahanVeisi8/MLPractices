{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","\n","\n","class node:\n","  def __init__(self ,parent, left, right , values , column, threshhold):\n","    self.parent:node = parent\n","    self.left:node  = left\n","    self.right:node = right\n","    self.values:np.ndarray = values\n","    self.column:int = column\n","    self.threshhold:float = threshhold\n","\n","  def is_leaf(self):\n","    if self.values is None:\n","      return False\n","    return True"],"metadata":{"id":"o2CNQkqhIHqX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ZRoK_wVGsVx"},"outputs":[],"source":["class DecisionTree:\n","    # Multiclass Decision Tree classifier\n","\n","    def __init__(self, min_samples_split=2, max_depth=5, n_features=None ):\n","        \"\"\"\n","        Initialize the Decision Tree model.\n","        \"\"\"\n","        #my code\n","        self.n_features = n_features\n","        self.max_depth=max_depth\n","        self.min_sample_split=min_sample_split\n","        #\n","        self.tree = node(None, None, None, None, None, None) # self.root=None\n","        self.number_of_classes = -1\n","        self.classes = None\n","\n","    def fit(self, X, Y, max_depth=5, kind='best', sample_size=20):\n","      #To DO\n","        \"\"\"\n","        Fit the Decision Tree model to the provided data.\n","\n","        Args:\n","        - X: Input features.\n","        - Y: Labels.\n","        - max_depth: Maximum depth of the decision tree.\n","        - kind: 'best' for best split(check all the possible splits),\n","         'random' for random split(check sample_size candidates for the splits on each column).\n","        - sample_size: Number of random samples to consider for splitting (for 'random' kind).\n","\n","        Returns:\n","        - None\n","\n","        Example:\n","        >>> tree = DecisionTree()\n","        >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n","        >>> Y = np.array([0, 1, 2])\n","        >>> tree.fit(X, Y, max_depth=5, kind='best', sample_size=20)\n","        \"\"\"\n","        #my code\n","        self.n_features=X.shape[1] if not x.n_features else min(X.shape[1],x.n_features)\n","        #\n","        self.classes = np.unique(Y)\n","        self.number_of_classes = len(self.classes)\n","        self.tree = self._tree_builder(self.tree, X, Y, max_depth, kind, sample_size) #self.root = self._grow_tree(X, y)\n","\n","    def predict(self, X):\n","      #To DO\n","        \"\"\"\n","        Predict class labels for input data.\n","\n","        Args:\n","        - X: Input features for prediction.\n","\n","        Returns:\n","        - Predicted class labels.\n","\n","        Example:\n","        >>> tree = DecisionTree()\n","        >>> X = np.array([[2, 3], [3, 4]])\n","        >>> predicted_labels = tree.predict(X)\n","        \"\"\"\n","        predicted_labels = []\n","        for x in X:\n","            label = self._predict_tree(self.tree, x)\n","            predicted_labels.append(label)\n","        return np.array(predicted_labels)\n","\n","    def _predict_tree(self, tree, X):\n","      #To DO\n","        \"\"\"\n","        Predict class labels for input data using the decision tree.\n","        This function can be used recursively to buid the predictions.\n","\n","        Args:\n","        - tree: Current decision tree node.\n","        - X: Input data for prediction.\n","\n","        Returns:\n","        - Predicted class labels.\n","\n","        Example:\n","        >>> tree = DecisionTree()\n","        >>> node = tree.tree\n","        >>> X = np.array([[2, 3], [3, 4]])\n","        >>> predicted_labels = tree._predict_tree(node, X)\n","        \"\"\"\n","        #my code\n","\n","        #\n","        if tree.is_leaf():\n","            return np.argmax(tree.values)\n","        if X[tree.column] <= tree.threshold:\n","            return self._predict_tree(tree.left, X)\n","        else:\n","            return self._predict_tree(tree.right, X)\n","\n","    def _tree_builder(self, tree, X, Y, depth):\n","      #To DO\n","        \"\"\"\n","        Build the Decision Tree recursively.\n","\n","        Args:\n","        - tree: Current decision tree node.\n","        - X: Input features for the current node.\n","        - Y: Labels for the current node.\n","        - depth: Current depth in the tree.\n","\n","        Returns:\n","        - None\n","\n","        Example:\n","        >>> tree = DecisionTree()\n","        >>> node = tree.tree\n","        >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n","        >>> Y = np.array([0, 1, 0])\n","        >>> tree._tree_builder(node, X, Y, depth=1)\n","        \"\"\"\n","        if depth == 0 or len(np.unique(Y)) == 1:\n","            self._build_leaf(tree, Y)\n","        else:\n","            if kind == 'best':\n","                best_row, best_column, best_threshold = self._find_split(X, Y)\n","            elif kind == 'random':\n","                best_row, best_column, best_threshold = self._find_random_split(X, Y, sample_size)\n","            left_indices = X[:, best_column] <= best_threshold\n","            right_indices = X[:, best_column] > best_threshold\n","            left_node = node(tree, None, None, None, None, None)\n","            right_node = node(tree, None, None, None, None, None)\n","            tree.column = best_column\n","            tree.threshold = best_threshold\n","            tree.left = left_node\n","            tree.right = right_node\n","            self._tree_builder(left_node, X[left_indices], Y[left_indices], depth - 1, kind, sample_size)\n","            self._tree_builder(right_node, X[right_indices], Y[right_indices], depth - 1, kind, sample_size)\n","\n","\n","    def _check_leaf(self, Y):\n","        # is_leaf = self.tree.is_leaf(Y)\n","        # return is_leaf\n","        return len(np.unique(Y)) == 1\n","\n","    def _build_leaf(self, node, Y):\n","      #To DO\n","        \"\"\"\n","        Build a leaf node and assign class probabilities to node.values\n","        according to Y values in this node.\n","        The values must be an np.ndarray which the first index is\n","        the probability of the sample in this node to belong to\n","        class with label=self.classes[0] and so on for other indexes.\n","\n","        Args:\n","        - node: Current decision tree node which is going to be a leaf.\n","        - Y: Labels for the current node.\n","\n","        Returns:\n","        - None\n","\n","        Example:\n","        >>> tree = DecisionTree()\n","        >>> node = tree.tree\n","        >>> Y = np.array([0, 1, 0])\n","        >>> tree._build_leaf(node, Y)\n","        \"\"\"\n","        node.values = np.zeros(self.number_of_classes)\n","        for y in Y:\n","            node.values[y] += 1\n","        node.values /= len(Y)\n","\n","    def _calculate_entropy(self, labels):\n","      #To DO\n","        \"\"\"\n","        Calculate the entropy of a set of labels.\n","\n","        Args:\n","        - labels: Input labels.\n","\n","        Returns:\n","        - Entropy value.\n","\n","        Example:\n","        >>> tree = DecisionTree()\n","        >>> labels = np.array([0, 1, 0, 1, 1])\n","        >>> entropy = tree._calculate_entropy(labels)\n","        \"\"\"\n","        unique_labels, counts = np.unique(labels, return_counts=True)\n","        probabilities = counts / len(labels)\n","        entropy = -np.sum(probabilities * np.log2(probabilities))\n","        return entropy\n","\n","    def _information_gain(self, X, Y, column, row):\n","      #To DO\n","        \"\"\"\n","        Calculate information gain for a potential split.\n","\n","        Args:\n","        - X: Input features for the split.\n","        - Y: Labels for the split.\n","        - column: Column index for the split.\n","        - row: row index for the split.\n","\n","        Returns:\n","        - Information gain value.\n","\n","        Example:\n","        >>> tree = DecisionTree()\n","        >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n","        >>> Y = np.array([0, 1, 0])\n","        >>> gain = tree._information_gain(X, Y, column=1, threshold=2)\n","        \"\"\"\n","        total_entropy = self._calculate_entropy(Y)\n","        left_indices = X[:, column] <= threshold\n","        right_indices = X[:, column] > threshold\n","        left_entropy = self._calculate_entropy(Y[left_indices])\n","        right_entropy = self._calculate_entropy(Y[right_indices])\n","        weighted_entropy = (len(Y[left_indices]) / len(Y)) * left_entropy + (len(Y[right_indices]) / len(Y)) * right_entropy\n","        information_gain = total_entropy - weighted_entropy\n","        return information_gain\n","\n","    def _find_split(self, X, Y):\n","      #To DO\n","        \"\"\"\n","        Find the best or best random split for the decision tree.\n","        Remember that there are two versions of this function according to the\n","        'kind' and 'sample_size' arguments that are provided to the constructor.\n","        The random samples must be in the range of the values in the dataset and\n","        they must be unique. The sample_size is an upper bound, as\n","        the unique values might be less than sample_size.\n","\n","\n","        Args:\n","        - X: Input features for the split.\n","        - Y: Labels for the split.\n","\n","        Returns:\n","        - Tuple (best_row, best_column) representing the chosen split.\n","\n","        Example:\n","        >>> tree = DecisionTree()\n","        >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n","        >>> Y = np.array([0, 1, 0])\n","        >>> split = tree._find_split(X, Y)\n","        \"\"\"\n","        best_information_gain = 0\n","        best_column = 0\n","        best_threshold = 0\n","        for column in range(X.shape[1]):\n","            unique_values = np.unique(X[:, column])\n","            for threshold in unique_values:\n","                information_gain = self._information_gain(X, Y, column, threshold)\n","                if information_gain > best_information_gain:\n","                    best_information_gain = information_gain\n","                    best_column = column\n","                    best_threshold = threshold\n","        return best_column, best_threshold\n","\n","\n","    def _find_random_split(self, X, Y, sample_size):\n","        best_information_gain = 0\n","        best_column = 0\n","        best_threshold = 0\n","        for _ in range(sample_size):\n","            column = np.random.randint(0, X.shape[1])\n","            threshold = np.random.choice(np.unique(X[:, column]))\n","            information_gain = self._information_gain(X, Y, column, threshold)\n","            if information_gain > best_information_gain:\n","                best_information_gain = information_gain\n","                best_column = column\n","                best_threshold = threshold\n","        return best_column, best_threshold"]},{"cell_type":"code","source":[],"metadata":{"id":"v_2_KcQOIFyT"},"execution_count":null,"outputs":[]}]}